{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af33ff97-4dd7-4a96-96fd-7dd04a76eaeb",
   "metadata": {},
   "source": [
    "# テキスト分類器の学習\n",
    "予測までの流れとして、\n",
    "\n",
    "トークンエンコーディング > トークン埋め込み > エンコーダ > 隠れ層 > 分類ヘッド > 予測\n",
    "\n",
    "となる。  \n",
    "※今までのがトークンエンコーディング  \n",
    "\n",
    "事項からは、DistilBERTで以下両方を検討し、そのトレードオフを検証する  \n",
    "- 特徴抽出\n",
    "  - 隠れ状態を特徴として利用し、分類器を学習する。事前学習済みモデルは更新しない。\n",
    "- ファインチューニング\n",
    "  - モデル全体をend-to-endで学習する。事前学習済みモデルも同時に更新する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80b1652-44e4-406c-84aa-bc037ff85c3b",
   "metadata": {},
   "source": [
    "## 特徴抽出器としてのTransformer\n",
    "\n",
    "ボディの重みを変えず、分類器のみを学習する。  \n",
    "GPUなしで素早く学習できるが、浅いモデルとなり、勾配に依存するモデルには不向き。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8701bf71-11b9-4828-a065-3b7203836df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 1.81MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:25<00:00, 10.4MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)\n",
    "# model_ckptを指定し、指定したデバイスへモデルを読み込んでいる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bdb721-bd8e-4d35-83c2-1ebc80bfb98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIPS: TensorFlowあるいはPyTorchの重みのみリリースされている場合、フレームワーク間の変換が可能\n",
    "# 以下はPyTorchのみしかリリースされていないxlm-roberta-baseをTensorFlowへ変換する例\n",
    "cache = \"\"\"\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "tf_model = TFAutoModel.from_pretrained(model_ckpt)\n",
    "tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\")\n",
    "tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\", from_pt=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4655f472-0a35-4472-9eb0-fa31173a32c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 182kB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 690kB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 899kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93288a9e-7d95-4d37-b833-fe35233dd4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"this is a test\"\"\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# return_tensors ... PyTorchテンソルに変換する。必須。\n",
    "\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")\n",
    "# テンソルは [batch_size, n_tokens] という形状で得られる。\n",
    "\n",
    "inputs\n",
    "# inputs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35722669-9638-4d4b-a907-3973846d7c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 2023, 2003, 1037, 3231,  102]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for k, v in inputs.items():\n",
    "    print(k, v)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b58ad2-d213-4e39-8f80-dfcbb16101ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelと同じdeviceに、PyTorchテンソルを設置する。\n",
    "inputs = {k:v.to(device) for k, v in inputs.items()} # items() ... Dict型の要素をキーと値ペアの配列に変換する。順不同。\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c266fe-45b5-4f43-99d4-d73b36d0dc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[-0.1565, -0.1862,  0.0528,  ..., -0.1188,  0.0662,  0.5470],\n",
       "         [-0.3575, -0.6484, -0.0618,  ..., -0.3040,  0.3508,  0.5221],\n",
       "         [-0.2772, -0.4459,  0.1818,  ..., -0.0948, -0.0076,  0.9958],\n",
       "         [-0.2841, -0.3917,  0.3753,  ..., -0.2151, -0.1173,  1.0526],\n",
       "         [ 0.2661, -0.5094, -0.3180,  ..., -0.4203,  0.0144, -0.2149],\n",
       "         [ 0.9441,  0.0112, -0.4714,  ...,  0.1439, -0.7288, -0.1619]]],\n",
       "       device='cuda:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "outputs\n",
    "# torch.no_grad()は勾配の自動計算を無効にしている。 計算のメモリ使用量を減らす事が出来る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64382b9d-0f38-42d2-a49f-d9a2fbfadee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outputsには 隠れ状態、損失、アテンション等のオブジェクトが含まれる\n",
    "# 現在のモデルには、最後の隠れ状態である１属性のみを返す\n",
    "outputs.last_hidden_state.size()\n",
    "# [batch_size, n_tokens, hidden_dim] という形をしている。\n",
    "# ∴1バッチ、6個の入力トークン、768次元のベクトルということになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1397c42b-c6ed-424f-8ac8-4d951c81ad40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分類タスクの場合、入力特徴として[CLS]トークンに関連付けられた隠れ状態だけを使うのが一般的\n",
    "# これは各系列の最初に現れるので、このように抽出可能\n",
    "outputs.last_hidden_state[:, 0].size() # = [:][0]の意、配列指定の:はすべての行の取得、カンマは次元の区切りを表す。= [:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5d71da-bc95-43b6-b3bf-00c0c9cb6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# これまでの処理をラップ\n",
    "def extract_hidden_states(batch):\n",
    "    # モデルの入力をGPU上に配置\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "             if k in tokenizer.model_input_names} # 今回の場合、input_idsやattention_maskが含まれていることが条件\n",
    "    # 最後の隠れ状態を抽出\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state #**inputsは可変長キーワード引数を表す。input_ids, attention_maskなどのキーをまとめて渡せる。\n",
    "    # [CLS]トークンに対するベクトルを返す\n",
    "    return {\"hidden_state\": last_hidden_state[:, 0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73a40c91-2bb4-4c58-b3f1-6751c237f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 3.97k/3.97k [00:00<00:00, 25.5MB/s]\n",
      "Downloading metadata: 100%|██████████| 3.28k/3.28k [00:00<00:00, 20.2MB/s]\n",
      "Downloading readme: 100%|██████████| 8.78k/8.78k [00:00<00:00, 38.1MB/s]\n",
      "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Downloading data: 100%|██████████| 592k/592k [00:00<00:00, 8.35MB/s]\n",
      "Downloading data files:  33%|███▎      | 1/3 [00:00<00:01,  1.07it/s]\n",
      "Downloading data: 100%|██████████| 74.0k/74.0k [00:00<00:00, 5.92MB/s]\n",
      "Downloading data files:  67%|██████▋   | 2/3 [00:01<00:00,  1.24it/s]\n",
      "Downloading data: 100%|██████████| 74.9k/74.9k [00:00<00:00, 7.81MB/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:03<00:00,  1.00s/it]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 277.33it/s]\n",
      "Generating train split: 100%|██████████| 16000/16000 [00:00<00:00, 61921.17 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 58945.19 examples/s]\n",
      "Generating test split: 100%|██████████| 2000/2000 [00:00<00:00, 61283.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "emotions = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f4cb5c-fc60-4336-9d71-a40e9ee8e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3be6d9a0-bccd-47a0-aaab-c48a25d0690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 18314.21 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 34761.78 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 35032.67 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "emotions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1363f4ea-7907-4385-8f5d-28dd893fd1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_ids と attension_maskの列をtorch形式に変換\n",
    "emotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "emotions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee178220-3cf2-48f6-b53b-b8aca1a72b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:25<00:00, 636.33 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:02<00:00, 774.71 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:02<00:00, 802.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask', 'hidden_state'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask', 'hidden_state'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask', 'hidden_state'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 隠れ状態を一度に抽出（予測）\n",
    "emotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True) # batch_size=Noneを設定していない場合、デフォルトの1000が使用される\n",
    "emotions_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f7fa910-7f62-47a5-b4a6-54953bef92d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label', 'input_ids', 'attention_mask', 'hidden_state']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions_hidden[\"train\"].column_names # hidden_stateが追加されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5b85d-e1c2-4e06-a781-149e87585940",
   "metadata": {},
   "source": [
    "## 特徴行列の作成\n",
    "前処理されたデータセットを元に、隠れ状態を入力特徴量として、ラベルをターゲットとして使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a252a16-6789-4cb1-84fb-6ade61416df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 768), (2000, 768))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1fcb5-e9cd-467f-a8a2-30dec914ca40",
   "metadata": {},
   "source": [
    "## 学習データセットの可視化\n",
    "768次元の隠れ状態の可視化は難しいので、UMAPアルゴリズムを使って2次元に射影するとのこと。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a18b30b4-699e-4b38-8f36-d0b3138d3fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.196945</td>\n",
       "      <td>6.377566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.289124</td>\n",
       "      <td>5.459541</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.344516</td>\n",
       "      <td>3.050781</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.531262</td>\n",
       "      <td>3.600675</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.565280</td>\n",
       "      <td>3.578017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X         Y  label\n",
       "0  4.196945  6.377566      0\n",
       "1 -3.289124  5.459541      0\n",
       "2  5.344516  3.050781      3\n",
       "3 -2.531262  3.600675      2\n",
       "4 -3.565280  3.578017      3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 特徴を[0,1]区間にスケール\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# UMAPの初期化とfit\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# 2次元埋め込みのDataFrameを作成\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f604069e-7040-40c4-bc0e-4ba1aa17537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO_↑コードの理解と次コードでの学習データセットUMAP可視化"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

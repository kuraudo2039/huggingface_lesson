{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "281c7380-a1d9-42fb-b504-7cecd1d1d700",
   "metadata": {},
   "source": [
    "# Tokenize\n",
    "テキストをトークン化して、Transformerで処理可能な形式にする。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba7b95-50a6-478d-9e87-ce158ad4dde3",
   "metadata": {},
   "source": [
    "## 文字トークン化\n",
    "最も単純なトークン化手法の一つで、各文字をモデルに与える。  \n",
    "スペルミスや珍しい単語の処理に役立つ手法だが、デメリットとしてテキスト構造を無視してしまうことと、膨大な計算量、メモリ、データが必要になる等が挙げられる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b2bffc-01a1-4cfe-a7f5-5ef9b1e68a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24ed0c62-d6a2-4f4b-9856-62c1228bdc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'o',\n",
       " 'k',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 'z',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'a',\n",
       " 's',\n",
       " 'k',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'N',\n",
       " 'L',\n",
       " 'P',\n",
       " '.']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8441e0f-6e50-41dc-a335-92ede690d49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " 'L': 2,\n",
       " 'N': 3,\n",
       " 'P': 4,\n",
       " 'T': 5,\n",
       " 'a': 6,\n",
       " 'c': 7,\n",
       " 'e': 8,\n",
       " 'f': 9,\n",
       " 'g': 10,\n",
       " 'i': 11,\n",
       " 'k': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'r': 15,\n",
       " 's': 16,\n",
       " 't': 17,\n",
       " 'x': 18,\n",
       " 'z': 19}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "token2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09d9ccb1-c285-44e0-8d54-bbbbbb847cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5,\n",
       " 14,\n",
       " 12,\n",
       " 8,\n",
       " 13,\n",
       " 11,\n",
       " 19,\n",
       " 11,\n",
       " 13,\n",
       " 10,\n",
       " 0,\n",
       " 17,\n",
       " 8,\n",
       " 18,\n",
       " 17,\n",
       " 0,\n",
       " 11,\n",
       " 16,\n",
       " 0,\n",
       " 6,\n",
       " 0,\n",
       " 7,\n",
       " 14,\n",
       " 15,\n",
       " 8,\n",
       " 0,\n",
       " 17,\n",
       " 6,\n",
       " 16,\n",
       " 12,\n",
       " 0,\n",
       " 14,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 4,\n",
       " 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55bf01fb-38c7-4cb9-9e92-ac0ed1ed49af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([38, 20])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上記のままではただのid列で、各トークンのかけ合わせをベクトルとして表現できない\n",
    "# ∴one-hotベクトルに変換する必要がある。\n",
    "\"\"\"\n",
    "トークン: [c,b,a]\n",
    "ただのid: [2,1,0]\n",
    "one-hotベクトル: \n",
    "[\n",
    "    [0, 0, 1], # 2 = c\n",
    "    [0, 1, 0], # 1 = b\n",
    "    [1, 0, 0]  # 0 = a\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx)) # num_classesを設定しないとone-hotベクトルが語彙の大きさよりも短くなってしまう=不揃いなテンソルができるため、必ず設定する。\n",
    "one_hot_encodings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdb35cb2-c75f-4b6a-bb4f-e956ac8b2cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 5\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23379e56-51d2-4a52-9d8c-a11b84c1fa8c",
   "metadata": {},
   "source": [
    "## 単語トークン化\n",
    "文字毎ではなく、単語に分割する。  \n",
    "単語は活用形や句読点を含むものが別々のものとして扱われてしまい、膨大な数の語彙が存在することになり、その語彙の数だけone-hotベクトルの次元数が大きくなるということになる。  \n",
    "そのため次に挙げるサブワードトークンにより、文字トークン化と単語トークン化の良いとこどりをするのが一般的。  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c594bd23-5a6c-4f1e-8edd-6d0946e9f69d",
   "metadata": {},
   "source": [
    "tokenized_text = text.split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ea102-f81e-436d-87e8-59ece3552787",
   "metadata": {},
   "source": [
    "## サブワードトークン化\n",
    "頻出単語は一意なものとして、稀多喃語はより小さな単位に分割して処理するので、入力の長さを管理可能なサイズに保ちつつ、スペルミス等も処理できるようにする。  \n",
    "文字トークンや単語トークンと違い、トークン化時に統計的ルールとアルゴリズムを組み合わせて、事前学習用のコーパスからトークン化を学習する必要がある。  \n",
    "BERTやDistilBERTのトークナイザーではWordPieceというトークン化アルゴリズムが用いられている。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c4b311b4-c266-48a9-bfe9-cb09bb6461a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f126e72e-0067-4c6a-a79c-66d4388536d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32cd67da-4ed6-4586-9d41-a4f760a863b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "text = 'Tokenizing text is a core task of NLP.'\n",
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e5f1bd6f-bc80-4d98-81fd-3f41e3fa6386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7787d46f-c4bd-4518-84d7-7e93ea2fc0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] tokenizing text is a core task of nlp. [SEP]'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44b7e44b-8555-4238-99b8-4f5e7507a2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizerの語彙数\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "541b9382-5fa2-44ea-a751-b293dac13bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの最大コンテキストサイズ\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b60b5075-8c83-41f3-96b1-ce782ec003c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# モデルの入力層\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a455f8-5b01-4026-b5b5-95a656d031de",
   "metadata": {},
   "source": [
    "# データセット全体のトークン化\n",
    "DatasetDictオブジェクトのmap()メソッドを使用すると便利"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "96e8418b-df50-40a8-b6ae-892aed2462e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "# padding=True ... バッチ内で最大長のベクトルまでゼロで埋める。\n",
    "# truncation=True ... トークンをモデルの最大コンテキストサイズまでとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1cdf06cb-f146-41d8-99dc-1074d14ac615",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "emotions = load_dataset(\"dair-ai/emotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6696090-ac47-4d7b-a729-a04be42c5a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(emotions[\"train\"][:2])\n",
    "# 0はpaddingで追加されたトークン\n",
    "# attention-mask は、モデルが入力のパディング部分を無視できるように設定されている。∴attention-mask=0はパディング箇所"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d3ac929-deac-4511-82de-22eae988a997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'i',\n",
       " 'didn',\n",
       " '##t',\n",
       " 'feel',\n",
       " 'humiliated',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パディングしている\"0\"は、[PAD]トークンに該当する\n",
    "tokenizer.convert_ids_to_tokens(tokenize(emotions[\"train\"][:2])['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d73f7cb-0583-40be-ac97-f7bdbe5629da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 16000/16000 [00:00<00:00, 17198.91 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 33838.68 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 35143.33 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['text', 'label', 'input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# コーパスすべてをトークナイズし、バッチに分割\n",
    "emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)\n",
    "# batched=True ... 一括エンコード、デフォルトでは個別に動作することを適当に複数にまとめて処理するため、処理速度が上がるらしい。\n",
    "# batch_size=None ... emotionsデータセット全体を1バッチとして適用\n",
    "emotions_encoded[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e473956-6081-450a-b978-3f00a50a93d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
